{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7ee99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import cv2\n",
    "from diffusers import ControlNetModel, AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "import numpy as np\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1037cbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb991696491f41bdbbbda5901e786652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebdee8b365d40e4b6d8da936b6221db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\n",
    "             \"diffusers/controlnet-depth-sdxl-1.0\", # Smaller Depth ControlNet model\n",
    "            torch_dtype=torch.float16,\n",
    "            variant=\"fp16\", \n",
    "            use_safetensors=True\n",
    "        ).to(\"cuda\")\n",
    "sdxl = AutoPipelineForImage2Image.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-xl-base-1.0\", # Faster Turbo model\n",
    "            controlnet=controlnet,\n",
    "            torch_dtype = torch.float16,\n",
    "            variant = \"fp16\",\n",
    "            use_safetensors=True\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "depth_estimator = pipeline(\n",
    "    task=\"depth-estimation\",\n",
    "    model=\"Intel/zoedepth-nyu-kitti\",\n",
    "    dtype=torch.float32,\n",
    "    device=-1,\n",
    "    use_fast = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d14695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_base64(image: Image.Image, image_format: str = \"PNG\") -> str:\n",
    "    \"\"\"\n",
    "    Encode a PIL image to a base64 string.\n",
    "\n",
    "    Parameters:\n",
    "        image (Image.Image): The PIL image to encode.\n",
    "        image_format (str): Image format to use (\"PNG\", \"JPEG\", etc.)\n",
    "\n",
    "    Returns:\n",
    "        str: Base64-encoded image string.\n",
    "    \"\"\"\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=image_format)\n",
    "\n",
    "    img64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    if img64 is None:\n",
    "        raise TypeError(\"Please check this goddam function\")\n",
    "    \n",
    "    return img64 \n",
    "\n",
    "\n",
    "def normalize_depth(depth: np.ndarray, clip_percentiles=(1,99), invert=False):\n",
    "    lo, hi = np.percentile(depth, clip_percentiles)\n",
    "    d = np.clip(depth, lo, hi)\n",
    "    d = (d - d.min()) / (d.max() - d.min() + 1e-8)\n",
    "    if invert:\n",
    "        d = 1.0 - d\n",
    "    return d\n",
    "\n",
    "def depth_to_uint8_rgb(depth: np.ndarray, invert=False):\n",
    "    d = normalize_depth(depth, invert=invert)\n",
    "    img8 = (d * 255.0).astype(np.uint8)\n",
    "    rgb = np.stack([img8, img8, img8], axis=-1)\n",
    "    return Image.fromarray(rgb)\n",
    "\n",
    "def depth_to_uint16_rgb(depth: np.ndarray, invert=False):\n",
    "    d = normalize_depth(depth, invert=invert)\n",
    "    img16 = (d * 65535.0).astype(np.uint16)\n",
    "    # Pillow needs mode='I;16' handling per-channel; easier to save as single channel then convert in your pipeline\n",
    "    return img16\n",
    "\n",
    "def preprocess_img(img: Image):\n",
    "    \"\"\"Preprocesses the input image: Load, Grayscale, Resize, CLAHE, Denoise, Sharpen, Convert to RGB.\"\"\"\n",
    "    results = depth_estimator(img)[\"depth\"]\n",
    "    control_img = depth_to_uint8_rgb(results, invert=True) \n",
    "    img_np = np.array(control_img)\n",
    "    \n",
    "    # FIX: Convert the 3-channel RGB image to a single-channel grayscale image\n",
    "    gray_img = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Create the CLAHE object\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    \n",
    "    # Now, apply CLAHE to the grayscale image\n",
    "    contrast_img = clahe.apply(gray_img)\n",
    "    \n",
    "    final_image = Image.fromarray(contrast_img)\n",
    "    \n",
    "    \n",
    "    return final_image\n",
    "\n",
    "\n",
    "def base_64_to_pil(base64_str: str):\n",
    "    if ',' in base64_str:\n",
    "        base64_str = base64_str.split(',')[1]\n",
    "    \n",
    "    \n",
    "    image_data = base64.b64decode(base64_str)\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a3952b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer(url, steps, net, guid, res: int):\n",
    "    img = load_image(url).resize((res, res), Image.LANCZOS)\n",
    "    control_image = preprocess_img(img)\n",
    "    print('created control image...')\n",
    "    prompt = \"close-up studio portrait of a newborn baby, \" \\\n",
    "    \"asleep, head and shoulders, face angle and composition \" \\\n",
    "    \"follow the control image, tiny eyelashes, soft natural skin,\" \\\n",
    "    \" gentle rosy cheeks, subtle peach fuzz, relaxed lips, serene expression,\" \\\n",
    "    \" warm diffused softbox lighting, clean black background, \" \\\n",
    "    \"shallow depth of field, 85mm photo look, ultra-detailed realistic skin texture,\" \n",
    "   # \" high dynamic range, professional newborn photography, photorealistic, 1:1\"\n",
    "\n",
    "    negative_prompt = \"ultrasound texture, scan lines, speckle noise, medical imaging look,\" \\\n",
    "    \" clay or plastic skin, CGI, doll-like, uncanny, deformed or asymmetrical facial features, \" \\\n",
    "    \"extra fingers or limbs, multiple faces, open eyes, teeth, adult traits, stubble, \" \\\n",
    "    \"heavy makeup, jewelry, medical equipment, umbilical cord, placenta, \" \\\n",
    "    \"occluded face, hands covering face, harsh shadows, blown highlights, \" \\\n",
    "    \"over-sharpening halos, banding, text, watermark, logo, low-res, blurry, \" \\\n",
    "    \"oversaturated, cartoon, painting, anime\"\n",
    "\n",
    "    \n",
    "    num_inference_steps = steps\n",
    "    controlnet_conditioning_scale = net\n",
    "    guidance = guid\n",
    "\n",
    "\n",
    "    # Run batch if your pipeline supports it\n",
    "    output_images = []\n",
    "\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        tmp = sdxl(\n",
    "            image=img,\n",
    "            prompt= prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            control_image=control_image,\n",
    "            guidance_scale=guidance,\n",
    "            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "    \n",
    "            height=res,\n",
    "            width=res\n",
    "        ).images\n",
    "        output_images.append(tmp[0])\n",
    "        del tmp, control_image, img\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    # Convert to base64\n",
    "    output_images = [encode_base64(image) for image in output_images]\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return {\"images\": output_images}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45df052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "def before_after(url):\n",
    "    img = load_image(url).resize((512, 512), Image.LANCZOS)\n",
    "    imgs = infer(url, 15, 0.45,6.5, 512)\n",
    "    img_after = base_64_to_pil(imgs['images'][0])\n",
    "    return make_image_grid([img, img_after], 1,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa283c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created control image...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6380da2cdf164e6f91e21dd2d966e5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "before_after(\"https://raw.githubusercontent.com/josho201/3dUltra/refs/heads/main/imgs/test2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd79d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Inferless",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
